Splitting texts...
Creating filtered texts... This might take up to one minute.
********************************************************************************
********************************************************************************
********************************************************************************
Preparing test data
Test data was prepared
Training models using Plain texts.
********************************************************************************
train A SVM 
*********************************************************
report of SVM model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.234     0.054     0.088       722
    labels_2      0.444     0.006     0.011       722
    labels_3      0.111     0.001     0.003       713
    labels-4      0.198     0.971     0.329       681

    accuracy                          0.200      3520
   macro avg      0.197     0.206     0.086      3520
weighted avg      0.200     0.200     0.084      3520

********************************************************************************
Train B NB
*********************************************************
report of NB model: 
              precision    recall  f1-score   support

    labels-0      0.245     0.424     0.311       682
    labels_1      0.215     0.313     0.255       722
    labels_2      0.191     0.147     0.166       722
    labels_3      0.197     0.056     0.087       713
    labels-4      0.299     0.233     0.262       681

    accuracy                          0.233      3520
   macro avg      0.229     0.235     0.216      3520
weighted avg      0.228     0.233     0.215      3520

********************************************************************************
Train C RF
*********************************************************
report of Forest model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.000     0.000     0.000       722
    labels_2      0.000     0.000     0.000       722
    labels_3      0.000     0.000     0.000       713
    labels-4      0.194     1.000     0.324       681

    accuracy                          0.193      3520
   macro avg      0.039     0.200     0.065      3520
weighted avg      0.037     0.193     0.063      3520

********************************************************************************
Train D LR
*********************************************************
report of LogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.167     0.001     0.003       722
    labels_2      0.000     0.000     0.000       722
    labels_3      0.000     0.000     0.000       713
    labels-4      0.193     0.997     0.324       681

    accuracy                          0.193      3520
   macro avg      0.072     0.200     0.065      3520
weighted avg      0.072     0.193     0.063      3520

********************************************************************************
********************************************************************************
Training models using Filtered texts.
********************************************************************************
train A SVM 
*********************************************************
report of _fSVM model: 
              precision    recall  f1-score   support

    labels-0      0.236     0.145     0.180       702
    labels_1      0.189     0.238     0.211       672
    labels_2      0.232     0.119     0.157       717
    labels_3      0.216     0.135     0.166       721
    labels-4      0.220     0.444     0.294       708

    accuracy                          0.215      3520
   macro avg      0.219     0.216     0.202      3520
weighted avg      0.219     0.215     0.201      3520

********************************************************************************
Train B NB
*********************************************************
report of _fNB model: 
              precision    recall  f1-score   support

    labels-0      0.201     0.218     0.209       702
    labels_1      0.203     0.320     0.249       672
    labels_2      0.231     0.128     0.165       717
    labels_3      0.226     0.180     0.201       721
    labels-4      0.193     0.199     0.196       708

    accuracy                          0.208      3520
   macro avg      0.211     0.209     0.204      3520
weighted avg      0.211     0.208     0.203      3520

********************************************************************************
Train D LR
*********************************************************
report of _fLR model: 
              precision    recall  f1-score   support

    labels-0      0.243     0.120     0.160       702
    labels_1      0.185     0.188     0.186       672
    labels_2      0.270     0.046     0.079       717
    labels_3      0.225     0.057     0.091       721
    labels-4      0.218     0.674     0.329       708

    accuracy                          0.216      3520
   macro avg      0.228     0.217     0.169      3520
weighted avg      0.229     0.216     0.168      3520

********************************************************************************
Train C RF
*********************************************************
report of _fRF model: 
              precision    recall  f1-score   support

    labels-0      0.281     0.013     0.025       702
    labels_1      0.000     0.000     0.000       672
    labels_2      0.250     0.001     0.003       717
    labels_3      0.133     0.003     0.005       721
    labels-4      0.204     0.997     0.338       708

    accuracy                          0.204      3520
   macro avg      0.174     0.203     0.074      3520
weighted avg      0.175     0.204     0.075      3520

Scores
╒════╤════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name               │   Precision │   Recall │   F1 Measure │
╞════╪════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ SVM                │    19.7458  │  20.6318 │      8.61273 │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ NB                 │    22.9386  │  23.4634 │     21.6214  │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ Forest             │     3.87042 │  20      │      6.48571 │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ LogisticRegression │     7.19788 │  19.969  │      6.52932 │
╘════╧════════════════════╧═════════════╧══════════╧══════════════╛
F_Scores
╒════╤════════╤═════════════╤══════════╤══════════════╕
│    │ Name   │   Precision │   Recall │   F1 Measure │
╞════╪════════╪═════════════╪══════════╪══════════════╡
│  0 │ _fSVM  │     21.855  │  21.5996 │     20.1523  │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  1 │ _fNB   │     21.0986 │  20.9132 │     20.3951  │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  2 │ _fRF   │     17.362  │  20.2833 │      7.41547 │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  3 │ _fLR   │     22.8415 │  21.6756 │     16.9075  │
╘════╧════════╧═════════════╧══════════╧══════════════╛
********************************************************************************
********************************************************************************
with ngram- Train models without removing stop words.
********************************************************************************
********************************************************************************
with ngram-Train models after removing stop words.
********************************************************************************
Prepare test data
Test data was prepared
********************************************************************************
train A SVM 
*********************************************************
report of ngramSVM model: 
              precision    recall  f1-score   support

    labels-0      0.296     0.283     0.290       682
    labels_1      0.237     0.252     0.244       722
    labels_2      0.199     0.193     0.196       722
    labels_3      0.232     0.248     0.240       713
    labels-4      0.329     0.310     0.319       681

    accuracy                          0.256      3520
   macro avg      0.259     0.257     0.258      3520
weighted avg      0.258     0.256     0.257      3520

********************************************************************************
Train B NB
*********************************************************
report of ngramNB model: 
              precision    recall  f1-score   support

    labels-0      0.311     0.327     0.319       682
    labels_1      0.248     0.271     0.259       722
    labels_2      0.224     0.220     0.222       722
    labels_3      0.251     0.304     0.275       713
    labels-4      0.367     0.236     0.287       681

    accuracy                          0.272      3520
   macro avg      0.280     0.272     0.273      3520
weighted avg      0.279     0.272     0.272      3520

********************************************************************************
Train C RF
*********************************************************
report of ngramForest model: 
              precision    recall  f1-score   support

    labels-0      0.287     0.324     0.305       682
    labels_1      0.260     0.107     0.151       722
    labels_2      0.227     0.122     0.159       722
    labels_3      0.230     0.196     0.212       713
    labels-4      0.281     0.602     0.384       681

    accuracy                          0.266      3520
   macro avg      0.257     0.270     0.242      3520
weighted avg      0.256     0.266     0.240      3520

********************************************************************************
Train D LR
*********************************************************
report of ngramLogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.338     0.270     0.300       682
    labels_1      0.237     0.265     0.250       722
    labels_2      0.203     0.213     0.208       722
    labels_3      0.243     0.245     0.244       713
    labels-4      0.346     0.352     0.349       681

    accuracy                          0.268      3520
   macro avg      0.274     0.269     0.270      3520
weighted avg      0.272     0.268     0.269      3520

********************************************************************************
train A SVM 
*********************************************************
report of ngram_fSVM model: 
              precision    recall  f1-score   support

    labels-0      0.207     0.135     0.164       702
    labels_1      0.209     0.126     0.158       672
    labels_2      0.242     0.091     0.132       717
    labels_3      0.215     0.098     0.135       721
    labels-4      0.232     0.674     0.345       708

    accuracy                          0.225      3520
   macro avg      0.221     0.225     0.187      3520
weighted avg      0.221     0.225     0.187      3520

********************************************************************************
Train B NB
*********************************************************
report of ngram_fNB model: 
              precision    recall  f1-score   support

    labels-0      0.207     0.123     0.154       702
    labels_1      0.180     0.632     0.280       672
    labels_2      0.208     0.053     0.084       717
    labels_3      0.199     0.054     0.085       721
    labels-4      0.174     0.090     0.119       708

    accuracy                          0.185      3520
   macro avg      0.194     0.190     0.145      3520
weighted avg      0.194     0.185     0.143      3520

********************************************************************************
Train C RF
*********************************************************
report of ngram_fForest model: 
              precision    recall  f1-score   support

    labels-0      0.194     0.067     0.100       702
    labels_1      0.191     0.091     0.123       672
    labels_2      0.228     0.121     0.158       717
    labels_3      0.169     0.087     0.115       721
    labels-4      0.229     0.713     0.347       708

    accuracy                          0.217      3520
   macro avg      0.202     0.216     0.169      3520
weighted avg      0.202     0.217     0.169      3520

********************************************************************************
Train D LR
*********************************************************
report of ngram_fLogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.199     0.123     0.152       702
    labels_1      0.211     0.210     0.211       672
    labels_2      0.223     0.091     0.129       717
    labels_3      0.221     0.046     0.076       721
    labels-4      0.234     0.655     0.345       708

    accuracy                          0.224      3520
   macro avg      0.218     0.225     0.182      3520
weighted avg      0.218     0.224     0.182      3520

Scores
╒════╤═════════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name                    │   Precision │   Recall │   F1 Measure │
╞════╪═════════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ ngramSVM                │     25.881  │  25.7135 │      25.7771 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ ngramNB                 │     28.0216 │  27.1887 │      27.2581 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ ngramForest             │     25.7047 │  27.0198 │      24.1926 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ ngramLogisticRegression │     27.3563 │  26.91   │      27.0382 │
╘════╧═════════════════════════╧═════════════╧══════════╧══════════════╛
F_Scores
╒════╤═══════════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name                      │   Precision │   Recall │   F1 Measure │
╞════╪═══════════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ ngram_fSVM                │     22.0917 │  22.4935 │      18.6691 │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ ngram_fNB                 │     19.3681 │  19.0487 │      14.4599 │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ ngram_fForest             │     20.2342 │  21.5944 │      16.861  │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ ngram_fLogisticRegression │     21.771  │  22.4824 │      18.2443 │
╘════╧═══════════════════════════╧═════════════╧══════════╧══════════════╛
********************************************************************************
Fasttext Model
****************************************************************************************************
with TESTSIZE=0.4-------
train fasttext models with 1 gram
              precision    recall  f1-score   support

    labels-0      0.979     0.979     0.979       702
    labels_1      0.793     0.876     0.833       696
    labels_2      0.655     0.652     0.653       706
    labels_3      0.753     0.682     0.716       711
    labels-4      0.917     0.915     0.916       705

    accuracy                          0.820      3520
   macro avg      0.820     0.821     0.819      3520
weighted avg      0.819     0.820     0.819      3520

train fasttext models with 2 gram
              precision    recall  f1-score   support

    labels-0      0.983     0.987     0.985       702
    labels_1      0.858     0.875     0.866       696
    labels_2      0.723     0.752     0.737       706
    labels_3      0.784     0.748     0.765       711
    labels-4      0.928     0.911     0.919       705

    accuracy                          0.854      3520
   macro avg      0.855     0.855     0.855      3520
weighted avg      0.855     0.854     0.854      3520

train fasttext models with 3 gram
              precision    recall  f1-score   support

    labels-0      0.980     0.987     0.984       702
    labels_1      0.818     0.912     0.863       696
    labels_2      0.774     0.708     0.740       706
    labels_3      0.805     0.789     0.797       711
    labels-4      0.928     0.913     0.921       705

    accuracy                          0.862      3520
   macro avg      0.861     0.862     0.861      3520
weighted avg      0.861     0.862     0.860      3520

train fasttext models with 4 gram
              precision    recall  f1-score   support

    labels-0      0.975     0.986     0.980       702
    labels_1      0.780     0.915     0.842       696
    labels_2      0.758     0.670     0.711       706
    labels_3      0.817     0.765     0.790       711
    labels-4      0.920     0.918     0.919       705

    accuracy                          0.850      3520
   macro avg      0.850     0.851     0.849      3520
weighted avg      0.850     0.850     0.848      3520

train fasttext models with 5 gram
              precision    recall  f1-score   support

    labels-0      0.960     0.980     0.970       702
    labels_1      0.816     0.889     0.851       696
    labels_2      0.793     0.738     0.764       706
    labels_3      0.822     0.793     0.807       711
    labels-4      0.919     0.913     0.916       705

    accuracy                          0.863      3520
   macro avg      0.862     0.863     0.862      3520
weighted avg      0.862     0.863     0.862      3520

train fasttext models with 6 gram
              precision    recall  f1-score   support

    labels-0      0.949     0.981     0.965       702
    labels_1      0.784     0.899     0.838       696
    labels_2      0.827     0.691     0.753       706
    labels_3      0.826     0.802     0.814       711
    labels-4      0.904     0.918     0.911       705

    accuracy                          0.858      3520
   macro avg      0.858     0.858     0.856      3520
weighted avg      0.858     0.858     0.856      3520

train fasttext models with 7 gram
              precision    recall  f1-score   support

    labels-0      0.941     0.979     0.959       702
    labels_1      0.741     0.907     0.816       696
    labels_2      0.856     0.589     0.698       706
    labels_3      0.796     0.817     0.806       711
    labels-4      0.892     0.915     0.903       705

    accuracy                          0.841      3520
   macro avg      0.845     0.841     0.837      3520
weighted avg      0.845     0.841     0.836      3520

train fasttext models with 8 gram
              precision    recall  f1-score   support

    labels-0      0.935     0.979     0.956       702
    labels_1      0.741     0.894     0.810       696
    labels_2      0.821     0.610     0.700       706
    labels_3      0.809     0.772     0.790       711
    labels-4      0.873     0.919     0.896       705

    accuracy                          0.834      3520
   macro avg      0.836     0.835     0.830      3520
weighted avg      0.836     0.834     0.830      3520

********************************************************************************
---------------SVM With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of SVM with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.616     0.863     0.719       721
    labels_1      0.509     0.627     0.562       740
    labels_2      0.413     0.126     0.193       714
    labels_3      0.393     0.199     0.264       669
    labels-4      0.565     0.873     0.686       676

    accuracy                          0.539      3520
   macro avg      0.499     0.537     0.485      3520
weighted avg      0.500     0.539     0.487      3520

********************************************************************************
---------------TF-IDF With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of SVM with TF-IDF Indices model: 
              precision    recall  f1-score   support

    labels-0      0.223     0.354     0.273       721
    labels_1      0.208     0.281     0.239       740
    labels_2      0.170     0.130     0.148       714
    labels_3      0.189     0.103     0.133       669
    labels-4      0.198     0.136     0.161       676

    accuracy                          0.204      3520
   macro avg      0.198     0.201     0.191      3520
weighted avg      0.198     0.204     0.193      3520

********************************************************************************
---------------Fasttext With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of FastText with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.638     0.882     0.740       739
    labels_1      0.445     0.615     0.516       688
    labels_2      0.404     0.120     0.185       718
    labels_3      0.432     0.148     0.221       728
    labels-4      0.529     0.886     0.662       647

    accuracy                          0.523      3520
   macro avg      0.489     0.530     0.465      3520
weighted avg      0.490     0.523     0.461      3520

********************************************************************************
---------------Testing the best models on Weebit---------------------------
********************************************************************************
---------------With Fasttext---------------------------
              precision    recall  f1-score   support

    labels-0      0.421     0.523     0.467       700
    labels_1      0.156     0.091     0.115       646
    labels_2      0.422     0.114     0.180       807
    labels_3      0.309     0.762     0.440       789
    labels-4      0.691     0.178     0.283       629

    accuracy                          0.344      3571
   macro avg      0.400     0.334     0.297      3571
weighted avg      0.396     0.344     0.300      3571

********************************************************************************
---------------SVM With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of SVM with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.627     0.881     0.733       706
    labels_1      0.495     0.595     0.541       709
    labels_2      0.439     0.152     0.225       706
    labels_3      0.406     0.205     0.273       706
    labels-4      0.566     0.877     0.688       693

    accuracy                          0.541      3520
   macro avg      0.507     0.542     0.492      3520
weighted avg      0.506     0.541     0.491      3520

********************************************************************************
---------------Fasttext With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of FastText with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.327     0.219     0.262       292
    labels_1      0.167     0.173     0.170       248
    labels_2      0.400     0.013     0.026       297
    labels_3      0.000     0.000     0.000       314
    labels-4      0.239     0.824     0.370       278

    accuracy                          0.238      1429
   macro avg      0.227     0.246     0.166      1429
weighted avg      0.225     0.238     0.161      1429

